"""
Script para realizar b√∫squedas tem√°ticas personalizadas
Permite ejecutar m√∫ltiples b√∫squedas con diferentes temas
"""

from news_sources_scraper import NewsSourcesScraper
import json
from datetime import datetime


# Definir b√∫squedas tem√°ticas
BUSQUEDAS = [
    {
        'nombre': 'Inteligencia Artificial',
        'keywords': [
            'inteligencia artificial', 'IA', 'AI', 
            'machine learning', 'deep learning', 
            'GPT', 'LLM', 'ChatGPT', 'OpenAI', 
            'Anthropic', 'Claude', 'Gemini',
            'modelo de lenguaje', 'neural network',
            'aprendizaje autom√°tico'
        ]
    },
    {
        'nombre': 'China y la IA',
        'keywords': [
            'China', 'chino', 'Beijing', 'Shanghai',
            'Alibaba', 'Baidu', 'Tencent', 'Huawei',
            'IA China', 'AI China', 'tecnolog√≠a china',
            'DeepSeek', 'SenseTime'
        ]
    },
    {
        'nombre': 'Blockchain y Criptomonedas',
        'keywords': [
            'blockchain', 'bitcoin', 'ethereum',
            'criptomoneda', 'crypto', 'NFT',
            'Web3', 'DeFi', 'smart contract'
        ]
    },
    {
        'nombre': 'Rob√≥tica y Automatizaci√≥n',
        'keywords': [
            'robot', 'rob√≥tica', 'automatizaci√≥n',
            'automation', 'drones', 'veh√≠culos aut√≥nomos',
            'cobot', 'industrial automation'
        ]
    },
    {
        'nombre': 'Cambio Clim√°tico y Sostenibilidad',
        'keywords': [
            'cambio clim√°tico', 'sostenibilidad',
            'energ√≠a renovable', 'emisiones',
            'carbono', 'medio ambiente',
            'sustentabilidad', 'green tech'
        ]
    }
]


def ejecutar_busqueda_individual(scraper, busqueda_config):
    """
    Ejecuta una b√∫squeda individual
    """
    print(f"\n{'='*70}")
    print(f"üîç B√∫squeda: {busqueda_config['nombre']}")
    print(f"{'='*70}\n")
    
    resultado = scraper.generate_search_result(
        search_query=busqueda_config['nombre'],
        keywords=busqueda_config['keywords']
    )
    
    # Guardar resultado individual
    filename = f"busqueda_{busqueda_config['nombre'].lower().replace(' ', '_')}.json"
    scraper.save_results(resultado, filename)
    
    return resultado


def ejecutar_todas_las_busquedas():
    """
    Ejecuta todas las b√∫squedas configuradas
    """
    scraper = NewsSourcesScraper()
    
    print("=" * 70)
    print("   SCRAPER DE NOTICIAS - B√öSQUEDAS TEM√ÅTICAS")
    print("=" * 70)
    print(f"\nüìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìö Total de b√∫squedas a realizar: {len(BUSQUEDAS)}")
    print(f"üåê Fuentes configuradas: {len(NewsSourcesScraper.SOURCES)}\n")
    
    # Almacenar todos los resultados
    resultados_completos = []
    
    # Ejecutar cada b√∫squeda
    for i, busqueda in enumerate(BUSQUEDAS, 1):
        print(f"\n[{i}/{len(BUSQUEDAS)}] ", end='')
        
        resultado = ejecutar_busqueda_individual(scraper, busqueda)
        
        resultados_completos.append({
            'busqueda': busqueda['nombre'],
            'resultado': resultado
        })
        
        # Mostrar resumen
        print(f"\nüìä Resumen:")
        print(f"   ‚Ä¢ Hallazgos encontrados: {resultado['total_hallazgos']}")
        print(f"   ‚Ä¢ Fuentes exitosas: {resultado['fuentes_exitosas']}/{resultado['total_fuentes_consultadas']}")
        
        if resultado['total_hallazgos'] > 0:
            print(f"\n   üì∞ Top 3 art√≠culos:")
            for j, hallazgo in enumerate(resultado['hallazgos'][:3], 1):
                print(f"      {j}. {hallazgo['titulo'][:70]}...")
                print(f"         Fuente: {hallazgo['fuente']}")
    
    # Guardar todos los resultados en un solo archivo
    archivo_completo = {
        'timestamp': datetime.now().isoformat(),
        'total_busquedas': len(BUSQUEDAS),
        'busquedas': resultados_completos
    }
    
    with open('todas_las_busquedas.json', 'w', encoding='utf-8') as f:
        json.dump(archivo_completo, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*70}")
    print("‚úÖ TODAS LAS B√öSQUEDAS COMPLETADAS")
    print(f"{'='*70}")
    print(f"\nüíæ Archivos generados:")
    print(f"   ‚Ä¢ todas_las_busquedas.json (archivo consolidado)")
    for busqueda in BUSQUEDAS:
        filename = f"busqueda_{busqueda['nombre'].lower().replace(' ', '_')}.json"
        print(f"   ‚Ä¢ {filename}")
    
    print(f"\nüìä Resumen general:")
    total_hallazgos = sum(r['resultado']['total_hallazgos'] for r in resultados_completos)
    print(f"   ‚Ä¢ Total de hallazgos en todas las b√∫squedas: {total_hallazgos}")
    print(f"   ‚Ä¢ Promedio de hallazgos por b√∫squeda: {total_hallazgos / len(BUSQUEDAS):.1f}")


def ejecutar_busqueda_personalizada():
    """
    Permite al usuario ejecutar una b√∫squeda personalizada
    """
    import sys
    
    scraper = NewsSourcesScraper()
    
    print("=" * 70)
    print("   B√öSQUEDA PERSONALIZADA")
    print("=" * 70)
    print()
    
    # Solicitar tema de b√∫squeda
    tema = input("üìù Ingresa el tema de b√∫squeda: ").strip()
    
    if not tema:
        print("‚ùå Debes ingresar un tema")
        return
    
    # Solicitar palabras clave
    print("\nüîë Ingresa las palabras clave (separadas por comas):")
    keywords_input = input("   > ").strip()
    
    if not keywords_input:
        keywords = [tema]
    else:
        keywords = [k.strip() for k in keywords_input.split(',')]
    
    print(f"\nüîç Buscando art√≠culos sobre: {tema}")
    print(f"üìå Palabras clave: {', '.join(keywords)}\n")
    
    # Ejecutar b√∫squeda
    resultado = scraper.generate_search_result(
        search_query=tema,
        keywords=keywords
    )
    
    # Guardar resultado
    filename = f"busqueda_personalizada_{tema.lower().replace(' ', '_')}.json"
    scraper.save_results(resultado, filename)
    
    # Mostrar resultados
    print(f"\n{'='*70}")
    print("üìä RESULTADOS")
    print(f"{'='*70}")
    print(f"Total de hallazgos: {resultado['total_hallazgos']}")
    
    if resultado['total_hallazgos'] > 0:
        print(f"\nüì∞ Art√≠culos encontrados:\n")
        for i, hallazgo in enumerate(resultado['hallazgos'][:10], 1):
            print(f"{i}. {hallazgo['titulo']}")
            print(f"   Fuente: {hallazgo['fuente']}")
            print(f"   URL: {hallazgo['url']}")
            if hallazgo['descripcion']:
                print(f"   {hallazgo['descripcion'][:150]}...")
            print()


if __name__ == "__main__":
    import sys
    
    print("\n" + "="*70)
    print("   üï∑Ô∏è  SCRAPER DE NOTICIAS MULTI-FUENTE")
    print("="*70 + "\n")
    
    print("Opciones:")
    print("  1. Ejecutar todas las b√∫squedas predefinidas")
    print("  2. Ejecutar b√∫squeda personalizada")
    print("  3. Solo b√∫squeda de Inteligencia Artificial")
    print("  4. Solo b√∫squeda de China y la IA")
    print()
    
    opcion = input("Selecciona una opci√≥n (1-4): ").strip()
    
    if opcion == "1":
        ejecutar_todas_las_busquedas()
    
    elif opcion == "2":
        ejecutar_busqueda_personalizada()
    
    elif opcion == "3":
        scraper = NewsSourcesScraper()
        busqueda = BUSQUEDAS[0]  # Inteligencia Artificial
        resultado = ejecutar_busqueda_individual(scraper, busqueda)
        print(f"\n‚úÖ B√∫squeda completada: {resultado['total_hallazgos']} hallazgos encontrados")
    
    elif opcion == "4":
        scraper = NewsSourcesScraper()
        busqueda = BUSQUEDAS[1]  # China y la IA
        resultado = ejecutar_busqueda_individual(scraper, busqueda)
        print(f"\n‚úÖ B√∫squeda completada: {resultado['total_hallazgos']} hallazgos encontrados")
    
    else:
        print("‚ùå Opci√≥n no v√°lida")
        sys.exit(1)
    
    print("\n" + "="*70)
    print("‚úÖ Proceso finalizado")
    print("="*70 + "\n")
